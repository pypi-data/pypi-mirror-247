# generated by datamodel-codegen:
#   filename:  https://substrate.run/openapi.json
#   timestamp: 2023-12-21T15:19:07+00:00

from __future__ import annotations

from typing import Dict, List, Optional

from pydantic import BaseModel, Field
from typing_extensions import Annotated


class StableDiffusionInputImage(BaseModel):
    image_url: str
    """
    Input image URL to modify with a text prompt.
    """
    mask_image_url: Optional[str] = None
    """
    Mask of the input image to selectively modify.
    """
    prompt_strength: Annotated[Optional[float], Field(ge=0.0, le=1.0)] = 0.6
    """
    Controls the influence of the input prompt on the generated output.
    """


class StableDiffusionIn(BaseModel):
    prompt: str
    """
    Input prompt.
    """
    negative_prompt: Optional[str] = None
    """
    Negative input prompt.
    """
    image: Optional[StableDiffusionInputImage] = None
    """
    Image parameters to modify an existing image (image-to-image).
    """
    store: Optional[bool] = False
    """
    Return a hosted image URL instead of base64 encoded image data.
    """
    width: Optional[int] = 512
    """
    Width of output image, in pixels.
    """
    height: Optional[int] = 512
    """
    Height of output image, in pixels.
    """
    steps: Optional[int] = 4
    """
    Number of diffusion steps to run.
    """
    seed: Optional[int] = None
    """
    Random noise seed. Default is a random seed.
    """
    num_images: Optional[int] = 1
    """
    Number of images to generate.
    """


class StableDiffusionImage(BaseModel):
    uri: str
    """
    Base 64-encoded JPEG image bytes, or a hosted image url if `store` is enabled.
    """
    seed: int
    """
    The random noise seed used for generation.
    """


class StableDiffusionOut(BaseModel):
    data: List[StableDiffusionImage]
    """
    Generated outputs.
    """


class MistralPrompt(BaseModel):
    prompt: str
    """
    Prompt to generate completions for.
    """


class MistralIn(BaseModel):
    prompts: List[MistralPrompt]
    """
    Input prompts.
    """
    temperature: Annotated[Optional[float], Field(ge=0.0, le=2.0)] = 0.75
    """
    Sampling temperature to use. Higher values make the output more random; lower values make the output more deterministic.
    """
    max_tokens: Optional[int] = 800
    """
    Maximum number of tokens to generate.
    """
    num_completions: Optional[int] = 1
    """
    Number of completions to generate for each prompt.
    """


class MistralCompletions(BaseModel):
    completions: List[str]
    """
    Generated completion choices.
    """


class MistralOut(BaseModel):
    data: List[MistralCompletions]
    """
    Generated outputs.
    """


class WhisperIn(BaseModel):
    audio_url: str
    """
    Input audio URL.
    """
    prompt: Optional[str] = None
    """
    Prompt to guide model on contents of input audio and desired output.
    """
    language: Optional[str] = 'en'
    """
    Language of input audio in ISO-639-1 format.
    """
    align: Optional[bool] = False
    """
    Align transcription to produce more accurate sentence-level timestamps and word-level timestamps. An array of word segments will be included in each sentence segment.
    """
    diarize: Optional[bool] = False
    """
    Identify speakers for each segment. Speaker IDs will be added to each output segment.
    """


class WhisperWord(BaseModel):
    word: str
    """
    Text of word.
    """
    start: float
    """
    Start time of word, in seconds.
    """
    end: float
    """
    End time of word, in seconds.
    """
    speaker: Optional[str] = None
    """
    ID of speaker, if `diarize` is enabled.
    """


class WhisperSegment(BaseModel):
    text: str
    """
    Text of segment.
    """
    start: float
    """
    Start time of segment, in seconds.
    """
    end: float
    """
    End time of segment, in seconds.
    """
    speaker: Optional[str] = None
    """
    ID of speaker, if `diarize` is enabled.
    """
    words: Optional[List[WhisperWord]] = None
    """
    Aligned words, if `align` is enabled.
    """


class WhisperOut(BaseModel):
    data: List[WhisperSegment]
    """
    Transcribed sentence segments.
    """


class JinaDoc(BaseModel):
    text: str
    """
    Text to embed.
    """
    id: Optional[str] = None
    """
    Document id. Required when storing embedding.
    """
    metadata: Optional[Dict[str, str]] = None
    """
    Additional metadata to store in embedding document.
    """
    embed_metadata_keys: Optional[List[str]] = None
    """
    Contents of `metadata` included to generate embedding.
    """


class JinaIn(BaseModel):
    docs: List[JinaDoc]
    """
    Documents to embed.
    """
    store: Optional[str] = None
    """
    Vector store ID in which embedding will be stored.
    """


class JinaEmbedding(BaseModel):
    vector: List[float]
    """
    Embedding vector.
    """
    id: Optional[str] = None
    """
    Document id.
    """
    metadata: Optional[Dict[str, str]] = None
    """
    Additional metadata stored in embedding document.
    """


class JinaOut(BaseModel):
    data: List[JinaEmbedding]
    """
    Embedding results.
    """


class ClipDoc(BaseModel):
    text: Optional[str] = None
    """
    Text to embed.
    """
    image_url: Optional[str] = None
    """
    URL of image to embed.
    """
    id: Optional[str] = None
    """
    Document id. Required when storing embedding.
    """
    metadata: Optional[Dict[str, str]] = None
    """
    Additional metadata to store in embedding document.
    """
    embed_metadata_keys: Optional[List[str]] = None
    """
    Contents of `metadata` included to generate embedding.
    """


class ClipIn(BaseModel):
    docs: List[ClipDoc]
    """
    Documents to embed.
    """
    store: Optional[str] = None
    """
    Vector store ID in which embedding will be stored.
    """


class ClipEmbedding(BaseModel):
    vector: List[float]
    """
    Embedding vector.
    """
    id: Optional[str] = None
    """
    Document id.
    """
    metadata: Optional[Dict[str, str]] = None
    """
    Additional metadata stored in embedding document.
    """


class ClipOut(BaseModel):
    data: List[ClipEmbedding]
    """
    Embedding results.
    """
