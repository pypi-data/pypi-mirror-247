# -*- coding: utf-8 -*-
# @Time    : 2022/10/6 10:51
# @Author  : LiZhen
# @FileName: string_utils.py
# @github  : https://github.com/Lizhen0628
# @Description:

import os
import html
import pypinyin
import regex as re
import unicodedata
import urllib
import urllib.parse
import w3lib.html
from typing import List, Union
from opencc import OpenCC
from weathon.utils.char_utils import CharUtils
from weathon.utils.dictionary import Dictionary
from weathon.utils.string_converter import ConvertMap, Converter


def add_curr_dir(name):
    return os.path.join(os.path.dirname(__file__), name)


class StringUtils:

    @staticmethod
    def cut_sentences(para, drop_empty_line=True, strip=True, deduplicate=False, language='zh'):
        '''cut_sentences

        :param para: è¾“å…¥æ–‡æœ¬
        :param drop_empty_line: æ˜¯å¦ä¸¢å¼ƒç©ºè¡Œ
        :param strip: æ˜¯å¦å¯¹æ¯ä¸€å¥è¯åšä¸€æ¬¡strip
        :param deduplicate: æ˜¯å¦å¯¹è¿ç»­æ ‡ç‚¹å»é‡ï¼Œå¸®åŠ©å¯¹è¿ç»­æ ‡ç‚¹ç»“å°¾çš„å¥å­åˆ†å¥
        :return: sentences: list of str
        '''
        if deduplicate:
            para = re.sub(r"([ã€‚ï¼ï¼Ÿ\!\?])\1+", r"\1", para)

        if language == 'en':
            from nltk import sent_tokenize
            sents = sent_tokenize(para)
            if strip:
                sents = [x.strip() for x in sents]
            if drop_empty_line:
                sents = [x for x in sents if len(x.strip()) > 0]
            return sents
        else:
            para = re.sub('([ã€‚ï¼ï¼Ÿ\?!])([^â€â€™)\]ï¼‰ã€‘])', r"\1\n\2", para)  # å•å­—ç¬¦æ–­å¥ç¬¦
            para = re.sub('(\.{3,})([^â€â€™)\]ï¼‰ã€‘â€¦.])', r"\1\n\2", para)  # è‹±æ–‡çœç•¥å·
            para = re.sub('(\â€¦+)([^â€â€™)\]ï¼‰ã€‘â€¦.])', r"\1\n\2", para)  # ä¸­æ–‡çœç•¥å·
            para = re.sub('([ã€‚ï¼ï¼Ÿ\?!]|\.{3,}|\â€¦+)([â€â€™)\]ï¼‰ã€‘])([^ï¼Œã€‚ï¼ï¼Ÿ\?â€¦.])', r'\1\2\n\3', para)
            # å¦‚æœåŒå¼•å·å‰æœ‰ç»ˆæ­¢ç¬¦ï¼Œé‚£ä¹ˆåŒå¼•å·æ‰æ˜¯å¥å­çš„ç»ˆç‚¹ï¼ŒæŠŠåˆ†å¥ç¬¦\næ”¾åˆ°åŒå¼•å·åï¼Œæ³¨æ„å‰é¢çš„å‡ å¥éƒ½å°å¿ƒä¿ç•™äº†åŒå¼•å·
            para = para.rstrip()  # æ®µå°¾å¦‚æœæœ‰å¤šä½™çš„\nå°±å»æ‰å®ƒ
            # å¾ˆå¤šè§„åˆ™ä¸­ä¼šè€ƒè™‘åˆ†å·;ï¼Œä½†æ˜¯è¿™é‡Œæˆ‘æŠŠå®ƒå¿½ç•¥ä¸è®¡ï¼Œç ´æŠ˜å·ã€è‹±æ–‡åŒå¼•å·ç­‰åŒæ ·å¿½ç•¥ï¼Œéœ€è¦çš„å†åšäº›ç®€å•è°ƒæ•´å³å¯ã€‚
            sentences = para.split("\n")
            if strip:
                sentences = [sent.strip() for sent in sentences]
            if drop_empty_line:
                sentences = [sent for sent in sentences if len(sent.strip()) > 0]
            return sentences

    @staticmethod
    def clean_text(text, remove_url=False, email=False, weibo_at=False, stop_terms=("è½¬å‘å¾®åš",),
                   emoji=False, weibo_topic=False, deduplicate_space=True,
                   norm_url=False, norm_html=False, to_url=False,
                   remove_puncts=False, remove_tags=True, t2s=True, q2b=True, wx_emoji2word=True,
                   expression_len=(1, 6), linesep2space=False):

        """
        è¿›è¡Œå„ç§æ–‡æœ¬æ¸…æ´—æ“ä½œï¼Œå¾®åšä¸­çš„ç‰¹æ®Šæ ¼å¼ï¼Œç½‘å€ï¼Œemailï¼Œhtmlä»£ç ï¼Œç­‰ç­‰
        Args:
            text:è¾“å…¥æ–‡æœ¬
            remove_url:ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰æ˜¯å¦å»é™¤ç½‘å€
            email:ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰æ˜¯å¦å»é™¤email
            weibo_at:ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰æ˜¯å¦å»é™¤å¾®åšçš„\@ç›¸å…³æ–‡æœ¬
            stop_terms: å»é™¤æ–‡æœ¬ä¸­çš„ä¸€äº›ç‰¹å®šè¯è¯­ï¼Œé»˜è®¤å‚æ•°ä¸º("è½¬å‘å¾®åš",)
            emoji: ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰å»é™¤\[\]åŒ…å›´çš„æ–‡æœ¬ï¼Œä¸€èˆ¬æ˜¯è¡¨æƒ…ç¬¦å·
            weibo_topic:ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰å»é™¤##åŒ…å›´çš„æ–‡æœ¬ï¼Œä¸€èˆ¬æ˜¯å¾®åšè¯é¢˜
            deduplicate_space:ï¼ˆé»˜è®¤ä½¿ç”¨ï¼‰åˆå¹¶æ–‡æœ¬ä¸­é—´çš„å¤šä¸ªç©ºæ ¼ä¸ºä¸€ä¸ª
            norm_url:ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰è¿˜åŸURLä¸­çš„ç‰¹æ®Šå­—ç¬¦ä¸ºæ™®é€šæ ¼å¼ï¼Œå¦‚(%20è½¬ä¸ºç©ºæ ¼)
            norm_html: ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰è¿˜åŸHTMLä¸­çš„ç‰¹æ®Šå­—ç¬¦ä¸ºæ™®é€šæ ¼å¼ï¼Œå¦‚(\&nbsp;è½¬ä¸ºç©ºæ ¼)
            to_url:  ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰å°†æ™®é€šæ ¼å¼çš„å­—ç¬¦è½¬ä¸ºè¿˜åŸURLä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œç”¨äºè¯·æ±‚ï¼Œå¦‚(ç©ºæ ¼è½¬ä¸º%20)
            remove_puncts: ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰ç§»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·
            remove_tags: ï¼ˆé»˜è®¤ä½¿ç”¨ï¼‰ç§»é™¤æ‰€æœ‰htmlå—
            t2s: ï¼ˆé»˜è®¤ä½¿ç”¨ï¼‰ç¹ä½“å­—è½¬ä¸­æ–‡
            q2b: (é»˜è®¤ä½¿ç”¨) å…¨è§’è½¬åŠè§’
            expression_len: å‡è®¾è¡¨æƒ…çš„è¡¨æƒ…é•¿åº¦èŒƒå›´ï¼Œä¸åœ¨èŒƒå›´å†…çš„æ–‡æœ¬è®¤ä¸ºä¸æ˜¯è¡¨æƒ…ï¼Œä¸åŠ ä»¥æ¸…æ´—ï¼Œå¦‚[åŠ ä¸Šç‰¹åˆ«ç•ªå¤–èéº¦èŠ±å¼€æ—¶å…±äº”å†Œ]ã€‚è®¾ç½®ä¸ºNoneåˆ™æ²¡æœ‰é™åˆ¶
            linesep2space: ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰æŠŠæ¢è¡Œç¬¦è½¬æ¢æˆç©ºæ ¼
        Returns: æ¸…æ´—åçš„æ–‡æœ¬
        """

        text = StringUtils.as_text(text)
        # unicodeä¸å¯è§å­—ç¬¦
        # æœªè½¬ä¹‰
        text = re.sub(r"[\u200b-\u200d]", "", text)
        # å·²è½¬ä¹‰
        text = re.sub(r"(\\u200b|\\u200c|\\u200d)", "", text)
        # åå‘çš„çŸ›ç›¾è®¾ç½®
        if norm_url and to_url:
            raise Exception("norm_urlå’Œto_urlæ˜¯çŸ›ç›¾çš„è®¾ç½®")
        if norm_html:
            text = html.unescape(text)
        if to_url:
            text = urllib.parse.quote(text)
        if remove_tags:
            text = w3lib.html.remove_tags(text)
        if remove_url:
            try:
                URL_REGEX = re.compile(
                    r'(?i)http[s]?://(?:[a-zA-Z]|[0-9]|[#$%*-;=?&@~.&+]|[!*,])+',
                    re.IGNORECASE)
                text = re.sub(URL_REGEX, "", text)
            except:
                # sometimes lead to "catastrophic backtracking"
                zh_puncts1 = "ï¼Œï¼›ã€ã€‚ï¼ï¼Ÿï¼ˆï¼‰ã€Šã€‹ã€ã€‘"
                URL_REGEX = re.compile(
                    r'(?i)((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>' + zh_puncts1 + ']+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'".,<>?Â«Â»â€œâ€â€˜â€™' + zh_puncts1 + ']))',
                    re.IGNORECASE)
                text = re.sub(URL_REGEX, "", text)
        if norm_url:
            text = urllib.parse.unquote(text)
        if email:
            EMAIL_REGEX = re.compile(r"[-a-z0-9_.]+@(?:[-a-z0-9]+\.)+[a-z]{2,6}", re.IGNORECASE)
            text = re.sub(EMAIL_REGEX, "", text)
        if weibo_at:
            text = re.sub(r"(å›å¤)?(//)?\s*@\S*?\s*(:|ï¼š| |$)", " ", text)  # å»é™¤æ­£æ–‡ä¸­çš„@å’Œå›å¤/è½¬å‘ä¸­çš„ç”¨æˆ·å
        if wx_emoji2word:
            text = StringUtils.wechat_expression_2_word(text)
        if emoji:
            # å»é™¤æ‹¬å·åŒ…å›´çš„è¡¨æƒ…ç¬¦å·
            # ? lazy matché¿å…æŠŠä¸¤ä¸ªè¡¨æƒ…ä¸­é—´çš„éƒ¨åˆ†å»é™¤æ‰
            if type(expression_len) in {tuple, list} and len(expression_len) == 2:
                # è®¾ç½®é•¿åº¦èŒƒå›´é¿å…è¯¯ä¼¤äººç”¨çš„ä¸­æ‹¬å·å†…å®¹ï¼Œå¦‚[åŠ ä¸Šç‰¹åˆ«ç•ªå¤–èéº¦èŠ±å¼€æ—¶å…±äº”å†Œ]
                lb, rb = expression_len
                text = re.sub(r"\[\S{" + str(lb) + r"," + str(rb) + r"}?\]", "", text)
            else:
                text = re.sub(r"\[\S+?\]", "", text)
            # text = re.sub(r"\[\S+\]", "", text)
            # å»é™¤çœŸ,å›¾æ ‡å¼emoji
            emoji_pattern = re.compile("["
                                       u"\U0001F600-\U0001F64F"  # emoticons
                                       u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                       u"\U0001F680-\U0001F6FF"  # transport & map symbols
                                       u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                                       u"\U00002702-\U000027B0"
                                       "]+", flags=re.UNICODE)
            text = emoji_pattern.sub(r'', text)
        if weibo_topic:
            text = re.sub(r"#\S+#", "", text)  # å»é™¤è¯é¢˜å†…å®¹
        if linesep2space:
            text = text.replace("\n", " ")  # ä¸éœ€è¦æ¢è¡Œçš„æ—¶å€™å˜æˆ1è¡Œ
        if deduplicate_space:
            text = re.sub(r"(\s)+", r"\1", text)  # åˆå¹¶æ­£æ–‡ä¸­è¿‡å¤šçš„ç©ºæ ¼
        if t2s:
            text = StringUtils.traditional2simple(text)
        assert hasattr(stop_terms, "__iter__"), Exception("å»é™¤çš„è¯è¯­å¿…é¡»æ˜¯ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡")
        if type(stop_terms) == str:
            text = text.replace(stop_terms, "")
        else:
            for x in stop_terms:
                text = text.replace(x, "")
        if remove_puncts:
            text = StringUtils.remove_string_punc(text)
        if q2b:
            text = StringUtils.Q2B(text)
        return text.strip()

    @staticmethod
    def cut_sentences_simple(sentence: str) -> List[str]:
        sentence_delimiters = ['ã€‚', 'ï¼Ÿ', 'ï¼', 'â€¦']
        sents = []
        tmp = []
        for ch in sentence:  # éå†å­—ç¬¦ä¸²ä¸­çš„æ¯ä¸€ä¸ªå­—
            tmp.append(ch)
            if ch in sentence_delimiters:
                sents.append(''.join(tmp))
                tmp = []
        if len(tmp) > 0:  # å¦‚ä»¥å®šç•Œç¬¦ç»“å°¾çš„æ–‡æœ¬çš„æ–‡æœ¬ä¿¡æ¯ä¼šåœ¨å¾ªç¯ä¸­è¿”å›ï¼Œæ— éœ€å†æ¬¡ä¼ é€’
            sents.append(''.join(tmp))
        return sents

    @staticmethod
    def is_number_string(string: str) -> bool:
        """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦å…¨ä¸ºæ•°å­—"""
        return all(CharUtils.is_number(c) for c in string)

    @staticmethod
    def is_chinese_string(string: str) -> bool:
        """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦å…¨ä¸ºæ±‰å­—"""
        return all(CharUtils.is_chinese(c) for c in string)

    @staticmethod
    def is_eng_string(string: str) -> bool:
        """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦å…¨ä¸ºè‹±æ–‡"""
        return all(CharUtils.is_alphabet(c) for c in string)

    @staticmethod
    def is_control(ch: str) -> bool:
        """æ§åˆ¶ç±»å­—ç¬¦åˆ¤æ–­
        """
        return unicodedata.category(ch) in ('Cc', 'Cf')

    @staticmethod
    def is_special(ch: str) -> bool:
        """åˆ¤æ–­æ˜¯ä¸æ˜¯æœ‰ç‰¹æ®Šå«ä¹‰çš„ç¬¦å·
        """
        return bool(ch) and (ch[0] == '[') and (ch[-1] == ']')

    @staticmethod
    def escape_re_character(s: str) -> str:
        """å¯¹æ­£åˆ™å­—ç¬¦æ·»åŠ è½¬ä¹‰"""
        escape_char = ["\\", "\"", "*", ".", "?", "+", "$", "^", "[", "]", "(", ")", "{", "}", "|", "-"]
        for c in escape_char:
            if c in s:
                s = s.replace(c, re.escape(c))
        return s

    @staticmethod
    def B2Q(text: str) -> str:
        """
        å­—ç¬¦ä¸²åŠè§’è½¬å…¨è§’
        Args:
            text: è¾“å…¥å­—ç¬¦ä¸²

        Returns:è½¬åŒ–åå­—ç¬¦ä¸²
        """
        return "".join([CharUtils.B2Q(c) for c in text])

    @staticmethod
    def Q2B(text: str) -> str:
        """
        å­—ç¬¦ä¸²å…¨è§’è½¬åŠè§’
        Args:
            text: è¾“å…¥å­—ç¬¦ä¸²

        Returns:è½¬åŒ–åå­—ç¬¦ä¸²
        """
        return "".join([CharUtils.Q2B(c) for c in text])

    @staticmethod
    def as_text(text: str) -> Union[str, None]:
        """ç”Ÿæˆunicodeå­—ç¬¦ä¸²"""
        if text is None:
            return None
        elif isinstance(text, bytes):
            try:
                text = text.decode('utf-8')
            except UnicodeDecodeError:
                text = text.decode('gbk', 'ignore')
            return text
        elif isinstance(text, str):
            return text
        else:
            raise ValueError('Unknown type %r' % type(text))

    @staticmethod
    def wechat_expression_2_word(text: str) -> str:
        """å°†æ–‡æœ¬ä¸­çš„å¾®ä¿¡è¡¨æƒ…ä»£ç è½¬åŒ–æˆæ–‡å­—"""
        code_expression = Dictionary.wechat_expression()
        for key, value in code_expression.items():
            text = text.replace(key, value)
        return text

    @staticmethod
    def traditional2japanese(text: str) -> str:
        """
        æ—¥æ–‡æ–°å­—ä½“ è½¬åŒ– ä¸ºç¹ä½“
        """
        return OpenCC("jp2t").convert(text)

    @staticmethod
    def Japanese2traditional(text: str) -> str:
        """
        ç¹é«”ï¼ˆOpenCC æ¨™æº–ï¼ŒèˆŠå­—é«”ï¼‰åˆ°æ—¥æ–‡æ–°å­—é«”  ç¹ä½“è½¬åŒ–ä¸ºæ—¥æ–‡æ–°å­—ä½“
        """
        return OpenCC("t2jp").convert(text)

    @staticmethod
    def traditional2traditional(text: str, convert_type=""):
        """
        Args:
            text: åŸå§‹å­—ç¬¦ä¸²
            convert_type: è½¬åŒ–ç±»å‹
                t2tw.json Traditional Chinese (OpenCC Standard) to Taiwan Standard ç¹é«”ï¼ˆOpenCC æ¨™æº–ï¼‰åˆ°è‡ºç£æ­£é«”
                hk2t.json Traditional Chinese (Hong Kong variant) to Traditional Chinese é¦™æ¸¯ç¹é«”åˆ°ç¹é«”ï¼ˆOpenCC æ¨™æº–ï¼‰
                t2hk.json Traditional Chinese (OpenCC Standard) to Hong Kong variant ç¹é«”ï¼ˆOpenCC æ¨™æº–ï¼‰åˆ°é¦™æ¸¯ç¹é«”
                tw2t.json Traditional Chinese (Taiwan standard) to Traditional Chinese è‡ºç£æ­£é«”åˆ°ç¹é«”ï¼ˆOpenCC æ¨™æº–ï¼‰
        Returns:
        """
        assert len(convert_type) != 0, "convert_type is none, must be in ['t2tw','hk2t','t2hk','tw2t']"
        return OpenCC(convert_type).convert(text)

    @staticmethod
    def traditional2simple(text: str, convert_type="t2s") -> str:
        """
        å°†ç¹ä½“å­—ç¬¦ä¸²è½¬åŒ–ä¸ºç®€ä½“å­—ç¬¦ä¸²
        Args:
            text:
            convert_type:
                t2s.json Traditional Chinese to Simplified Chinese ç¹é«”åˆ°ç°¡é«”
                tw2s.json Traditional Chinese (Taiwan Standard) to Simplified Chinese è‡ºç£æ­£é«”åˆ°ç°¡é«”
                hk2s.json Traditional Chinese (Hong Kong variant) to Simplified Chinese é¦™æ¸¯ç¹é«”åˆ°ç°¡é«”
                tw2sp.json Traditional Chinese (Taiwan Standard) to Simplified Chinese with Mainland Chinese idiom ç¹é«”ï¼ˆè‡ºç£æ­£é«”æ¨™æº–ï¼‰åˆ°ç°¡é«”ä¸¦è½‰æ›çˆ²ä¸­åœ‹å¤§é™¸å¸¸ç”¨è©å½™
        """
        # word_map = Dictionary.traditional2simple_dic()
        # return Converter(ConvertMap(word_map)).convert(text)
        return OpenCC(convert_type).convert(text)

    @staticmethod
    def simple2traditional(text: str, convert_type="s2t") -> str:
        """
        å°†ç®€ä½“å­—ç¬¦ä¸²è½¬åŒ–ä¸ºç¹ä½“å­—ç¬¦ä¸²
        Args:
            text:
            convert_type:
                s2t.json Simplified Chinese to Traditional Chinese ç®€ä½“åˆ°ç¹é«”
                s2tw.json Simplified Chinese to Traditional Chinese (Taiwan Standard) ç°¡é«”åˆ°è‡ºç£æ­£é«”
                s2hk.json Simplified Chinese to Traditional Chinese (Hong Kong variant) ç°¡é«”åˆ°é¦™æ¸¯ç¹é«”
                s2twp.json Simplified Chinese to Traditional Chinese (Taiwan Standard) with Taiwanese idiom ç°¡é«”åˆ°ç¹é«”ï¼ˆè‡ºç£æ­£é«”æ¨™æº–ï¼‰ä¸¦è½‰æ›çˆ²è‡ºç£å¸¸ç”¨è©å½™
        """
        # word_map = Dictionary.simple2traditional_dic()
        # return Converter(ConvertMap(word_map)).convert(text)
        return OpenCC(convert_type).convert(text)

    @staticmethod
    def text2pinyin(text: str, with_tone: bool = False) -> List[str]:
        """
        å°†æ–‡æœ¬å­—ç¬¦ä¸²è½¬æˆæ‹¼éŸ³
        Args:
            text: è¾“å…¥çš„æ–‡æœ¬å­—ç¬¦ä¸²
            with_tone: æ˜¯å¦å¯¹è½¬åŒ–çš„æ‹¼éŸ³åŠ ä¸Šå£°è°ƒ
        Returns:
        """
        result = pypinyin.core.pinyin(text, strict=False, style=pypinyin.TONE if with_tone else pypinyin.NORMAL)
        return [_[0] for _ in result]

    @staticmethod
    def get_homophones_by_pinyin(input_pinyin: str) -> List[str]:
        """æ ¹æ®æ‹¼éŸ³å–æ‰€æœ‰åŒéŸ³å­—"""
        result = []
        # CJKç»Ÿä¸€æ±‰å­—åŒºçš„èŒƒå›´æ˜¯0x4E00-0x9FA5,ä¹Ÿå°±æ˜¯æˆ‘ä»¬ç»å¸¸æåˆ°çš„20902ä¸ªæ±‰å­—
        for i in range(0x4e00, 0x9fa6):
            if pypinyin.core.pinyin([chr(i)], style=pypinyin.NORMAL, strict=False)[0][0] == input_pinyin:
                result.append(chr(i))
        return result

    @staticmethod
    def remove_string_punc(s: str) -> str:
        """å»é™¤å­—ç¬¦ä¸²ä¸­çš„æ ‡ç‚¹ç¬¦å·"""
        s = re.compile("[ã€‚ï¼Œï¼ï¼Ÿã€ï¼›ï¼šâ€œâ€â€˜â€™ï¼ˆï¼‰ã€ã€‘\{\}ã€ã€ã€Œã€ã€”ã€•â€”â€”â€¦â€¦â€”\-ï½Â·ã€Šã€‹ã€ˆã€‰ï¹___\.]").sub("", s)
        s = re.compile(
            "[,\.\":\)\(\-!\?\|;'\$&/\[\]>%=#\*\+\\â€¢~@Â£Â·_\{\}Â©\^Â®`<â†’Â°â‚¬â„¢â€ºâ™¥â†Ã—Â§â€³â€²Ã‚â–ˆÂ½Ã â€¦â€œâ˜…â€â€“â—Ã¢â–ºâˆ’Â¢Â²Â¬â–‘Â¶â†‘Â±Â¿â–¾â•Â¦â•‘â€•Â¥â–“â€”â€¹â”€â–’ï¼šÂ¼âŠ•â–¼â–ªâ€ â– â€™â–€Â¨â–„â™«â˜†Ã©Â¯â™¦Â¤â–²Ã¨Â¸Â¾Ãƒâ‹…â€˜âˆâˆ™ï¼‰â†“ã€â”‚ï¼ˆÂ»ï¼Œâ™ªâ•©â•šÂ³ãƒ»â•¦â•£â•”â•—â–¬â¤Ã¯Ã˜Â¹â‰¤â€¡âˆš]").sub(
            "", s)
        return s


if __name__ == '__main__':
    # print(StringUtils.simple2traditional(
    #     "ç°ä»£ç¤¾ä¼šï¼Œå¾ˆå¤šäººéƒ½æƒ³æœ‰ä¸ªçº¢é¢œçŸ¥å·±ï¼Œä¹Ÿå°±æ˜¯å¼‚æ€§æœ‹å‹ï¼Œåœ¨è‡ªå·±å­¤ç‹¬å¯‚å¯æ—¶ï¼Œå¯ä»¥æœ‰ä¸ªå‡ºå£ã€‚åªæ˜¯å¼‚æ€§æœ‹å‹åœ¨ä¸€èµ·ï¼Œéš¾å…ä¼šå¼•äººéæƒ³ï¼Œæˆ–è®¸ä½ åªæ˜¯å•çº¯åœ°è®¤ä¸ºï¼Œæœ‰ä¸ªè¿™æ ·çš„æœ‹å‹ï¼Œèƒ½è®©è‡ªå·±å¯ä»¥æ”¾å¿ƒè¯‰è¯´å¿ƒäº‹ï¼Œè¿˜èƒ½å¤Ÿä¿æŒå•çº¯çš„æœ‹å‹å…³ç³»ã€‚"))
    # --------------- æµ‹è¯•æ–‡æœ¬æ¸…æ´— ------------------

    # ä¸å¯è§å­—ç¬¦
    # text1 = "æ§æ€ï¼å¹²å¾—æ¼‚äº®ï¼[doge] \\u200b\\u200b\\u200b"
    # text2 = StringUtils.clean_text(text1)
    # print("æ¸…æ´—å‰ï¼š", [text1])
    # print("æ¸…æ´—åï¼š", [text2])
    # assert text2 == "æ§æ€ï¼å¹²å¾—æ¼‚äº®ï¼"

    # ä¸¤ä¸ªè¡¨æƒ…ç¬¦å·ä¸­é—´æœ‰å†…å®¹
    # text1 = "#ç¼ºé’±æ‰¾æ–°æµª# çæ‰¾ä¸è‰¯ç½‘è´·ä¸å¦‚ç”¨æ–°æµªå®˜æ–¹å€Ÿæ¬¾ï¼Œä¸æŸ¥è´Ÿå€ºä¸å¡«è”ç³»äººã€‚  http://t.cn/A643boyi \næ–°æµª[æµª]ç”¨æˆ·ä¸“äº«ç¦åˆ©ï¼Œ[æµª]æ–°æµªäº§å“ç”¨çš„è¶Šä¹…é¢åº¦è¶Šé«˜ï¼Œå€Ÿä¸‡å…ƒæ—¥åˆ©ç‡æœ€ä½è‡³0.03%ï¼Œæœ€é•¿å¯åˆ†12æœŸæ…¢æ…¢è¿˜ï¼ http://t.cn/A643bojv  http://t.cn/A643bKHS \u200b\u200b\u200b"
    # text2 = StringUtils.clean_text(text1,remove_url=True,weibo_topic=True)
    # print("æ¸…æ´—å‰ï¼š", [text1])
    # print("æ¸…æ´—åï¼š", [text2])

    # åŒ…å«emoji
    text1 = "å„ä½å¤§ç¥ä»¬ğŸ™æ±‚æ•™ä¸€ä¸‹è¿™æ˜¯ä»€ä¹ˆåŠ¨ç‰©å‘€ï¼[ç–‘é—®]\n\nä¸ºä»€ä¹ˆå®ƒåŒæ—¶é•¿å¾—æœ‰ç‚¹å“äººåˆæœ‰ç‚¹å¯çˆ±[å…æ‚²]\n\n#thosetiktoks# http://t.cn/A6bXIC44 \u200b\u200b\u200b"
    text2 = StringUtils.clean_text(text1, emoji=True, remove_url=True)
    print("æ¸…æ´—å‰ï¼š", [text1])
    print("æ¸…æ´—åï¼š", [text2])
    # text1 = "JJæ£‹ç‰Œæ•°æ®4.3ä¸‡ã€‚æ•°æ®é“¾æ¥http://www.jj.cn/ï¼Œæ•°æ®ç¬¬ä¸€ä¸ªè´¦å·ï¼Œç¬¬äºŒä¸ªå¯†ç ï¼Œ95%å¯ç™»å½•ï¼Œå¯ä»¥ç™»å½•å®˜ç½‘æŸ¥çœ‹æ•°æ®æ˜¯å¦å‡†ç¡®"
    # text2 = StringUtils.clean_text(text1)
    # assert text2 == "JJæ£‹ç‰Œæ•°æ®4.3ä¸‡ã€‚æ•°æ®é“¾æ¥ï¼Œæ•°æ®ç¬¬ä¸€ä¸ªè´¦å·ï¼Œç¬¬äºŒä¸ªå¯†ç ï¼Œ95%å¯ç™»å½•ï¼Œå¯ä»¥ç™»å½•å®˜ç½‘æŸ¥çœ‹æ•°æ®æ˜¯å¦å‡†ç¡®"
