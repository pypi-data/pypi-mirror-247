# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: chimera_llm.proto
# Protobuf Python Version: 4.25.0
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder

# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n\x11\x63himera_llm.proto")\n\x0eInspectRequest\x12\x17\n\x0freport_duration\x18\x01 \x01(\r"\x7f\n\x0e\x41valiableModel\x12\x1e\n\x0bmodel_tagas\x18\x01 \x03(\x0e\x32\t.ModelTag\x12\x10\n\x08model_id\x18\x02 \x01(\r\x12\x12\n\nmodel_name\x18\x03 \x01(\t\x12\x0c\n\x04size\x18\x04 \x01(\t\x12\x19\n\x11model_description\x18\x05 \x01(\t"\x85\x01\n\x0fInspectResponse\x12)\n\x10\x61valiable_models\x18\x01 \x03(\x0b\x32\x0f.AvaliableModel\x12\x1f\n\x0e\x63urrent_status\x18\x02 \x01(\x0e\x32\x07.Status\x12&\n\rcurrent_model\x18\x03 \x01(\x0b\x32\x0f.AvaliableModel"?\n\x10LoadModelRequest\x12\x10\n\x08model_id\x18\x01 \x01(\r\x12\x19\n\x10json_model_param\x18\xe7\x07 \x01(\t"\\\n\x11LoadModelResponse\x12\x1f\n\x0e\x63urrent_status\x18\x01 \x01(\x0e\x32\x07.Status\x12&\n\rcurrent_model\x18\x02 \x01(\x0b\x32\x0f.AvaliableModel"~\n\rInferenceArgs\x12\x13\n\x0btemperature\x18\x01 \x01(\x02\x12\r\n\x05top_p\x18\x02 \x01(\x02\x12\x13\n\x0bmax_gen_len\x18\x03 \x01(\r\x12\x1a\n\x12repetition_penalty\x18\x04 \x01(\r\x12\x18\n\x0fjson_extra_args\x18\xe7\x07 \x01(\t"_\n\x11\x43ompletionRequest\x12\x12\n\nrequest_id\x18\x01 \x01(\t\x12\x0e\n\x06prompt\x18\x02 \x01(\t\x12&\n\x0einference_args\x18\x03 \x01(\x0b\x32\x0e.InferenceArgs"S\n\x14\x43ompletionPrediction\x12\x12\n\nrequest_id\x18\x01 \x01(\t\x12\x13\n\x0bresponse_id\x18\x02 \x01(\t\x12\x12\n\ngeneration\x18\x03 \x01(\t"3\n\x0b\x43hatMessage\x12\x13\n\x04role\x18\x01 \x01(\x0e\x32\x05.Role\x12\x0f\n\x07\x63ontent\x18\x02 \x01(\t"i\n\x0b\x43hatRequest\x12\x12\n\nrequest_id\x18\x01 \x01(\t\x12\x1e\n\x08messages\x18\x02 \x03(\x0b\x32\x0c.ChatMessage\x12&\n\x0einference_args\x18\x03 \x01(\x0b\x32\x0e.InferenceArgs"X\n\x0e\x43hatPrediction\x12\x12\n\nrequest_id\x18\x01 \x01(\t\x12\x13\n\x0bresponse_id\x18\x02 \x01(\t\x12\x1d\n\x07message\x18\x03 \x01(\x0b\x32\x0c.ChatMessage*M\n\x08ModelTag\x12\x16\n\x12MODEL_TYPE_UNKNOWN\x10\x00\x12\x0b\n\x07GENERAL\x10\x01\x12\x08\n\x04\x43HAT\x10\x02\x12\x08\n\x04TEXT\x10\x03\x12\x08\n\x04\x43ODE\x10\x04*B\n\x06Status\x12\x12\n\x0eSTATUS_UNKNOWN\x10\x00\x12\x0e\n\nINITLIZING\x10\x01\x12\t\n\x05READY\x10\x02\x12\t\n\x05\x45RROR\x10\x03*=\n\x04Role\x12\x10\n\x0cROLE_UNKNOWN\x10\x00\x12\n\n\x06SYSTEM\x10\x01\x12\x08\n\x04USER\x10\x02\x12\r\n\tASSISTANT\x10\x03\x32\xd1\x01\n\x03LLM\x12\x30\n\x07Inspect\x12\x0f.InspectRequest\x1a\x10.InspectResponse"\x00\x30\x01\x12\x34\n\tLoadModel\x12\x11.LoadModelRequest\x1a\x12.LoadModelResponse"\x00\x12\x39\n\nCompletion\x12\x12.CompletionRequest\x1a\x15.CompletionPrediction"\x00\x12\'\n\x04\x43hat\x12\x0c.ChatRequest\x1a\x0f.ChatPrediction"\x00\x62\x06proto3'
)

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "chimera_llm_pb2", _globals)
if _descriptor._USE_C_DESCRIPTORS == False:
    DESCRIPTOR._options = None
    _globals["_MODELTAG"]._serialized_start = 1048
    _globals["_MODELTAG"]._serialized_end = 1125
    _globals["_STATUS"]._serialized_start = 1127
    _globals["_STATUS"]._serialized_end = 1193
    _globals["_ROLE"]._serialized_start = 1195
    _globals["_ROLE"]._serialized_end = 1256
    _globals["_INSPECTREQUEST"]._serialized_start = 21
    _globals["_INSPECTREQUEST"]._serialized_end = 62
    _globals["_AVALIABLEMODEL"]._serialized_start = 64
    _globals["_AVALIABLEMODEL"]._serialized_end = 191
    _globals["_INSPECTRESPONSE"]._serialized_start = 194
    _globals["_INSPECTRESPONSE"]._serialized_end = 327
    _globals["_LOADMODELREQUEST"]._serialized_start = 329
    _globals["_LOADMODELREQUEST"]._serialized_end = 392
    _globals["_LOADMODELRESPONSE"]._serialized_start = 394
    _globals["_LOADMODELRESPONSE"]._serialized_end = 486
    _globals["_INFERENCEARGS"]._serialized_start = 488
    _globals["_INFERENCEARGS"]._serialized_end = 614
    _globals["_COMPLETIONREQUEST"]._serialized_start = 616
    _globals["_COMPLETIONREQUEST"]._serialized_end = 711
    _globals["_COMPLETIONPREDICTION"]._serialized_start = 713
    _globals["_COMPLETIONPREDICTION"]._serialized_end = 796
    _globals["_CHATMESSAGE"]._serialized_start = 798
    _globals["_CHATMESSAGE"]._serialized_end = 849
    _globals["_CHATREQUEST"]._serialized_start = 851
    _globals["_CHATREQUEST"]._serialized_end = 956
    _globals["_CHATPREDICTION"]._serialized_start = 958
    _globals["_CHATPREDICTION"]._serialized_end = 1046
    _globals["_LLM"]._serialized_start = 1259
    _globals["_LLM"]._serialized_end = 1468
# @@protoc_insertion_point(module_scope)
