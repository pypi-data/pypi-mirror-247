# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['gptsql']

package_data = \
{'': ['*']}

install_requires = \
['halo>=0.0.31,<0.0.32',
 'openai>=1.3.6,<2.0.0',
 'pandas>=2.1.3,<3.0.0',
 'prompt-toolkit>=3.0.41,<4.0.0',
 'psycopg2>=2.9.9,<3.0.0',
 'sqlalchemy>=2.0.23,<3.0.0',
 'tabulate>=0.9.0,<0.10.0',
 'termcolor>=2.3.0,<3.0.0',
 'toml>=0.10.2,<0.11.0']

entry_points = \
{'console_scripts': ['gptsql = gptsql.__main__:main']}

setup_kwargs = {
    'name': 'gptsql',
    'version': '0.1.7',
    'description': 'LLM-powered chat interface to your Postgres database',
    'long_description': '# gptsql\n\nAn LLM-powered chat interface to your database. This tool understands Postgres syntax and can easily translate English queries into proper SQL queries. Because of the wide training of the LLM model it can also infer relevant information about the structure and meaning of your tables and data. Uses an [Open AI](https://openai.com) model via the [Assistant API](https://platform.openai.com/docs/assistants/overview).\n\nThere are lots of tools around to enable "chat with your data" (mostly based on the [RAG](https://research.ibm.com/blog/retrieval-augmented-generation-RAG) architecture), but this is actually the quickest way to enable LLM chat with your data - no preparation is needed.\n\n\nHere\'s a quick demo showing natural language queries about the [IMDB](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/2QYZBT) movie database. You can even see the Assistant encounter SQL errors and then correct itself:\n\n\n\nhttps://github.com/tatari-tv/gptsql/assets/80947/7e946bdd-1ed9-4a03-9dc1-aca8d3a44926\n\n\n\n\n## Installation\n\nYou will need:\n\n1. credentials for your database\n2. an OpenAI [API Key](https://platform.openai.com/account/api-keys) from your OpenAI account.\n\nthen\n\n```\npip install gptsql\n```\n\nor download the source. \n\nRun the CLI with:\n\n    gptsql\n\nor use `python -m gptsql` to run from source.\n\n## What can it do?\n\nThen Open AI model understands most Postgres syntax, so it can generate both generic SQL commands as well as very Postgres-specific ones like querying system settings.\n\nThe LLM is also good at analyzing tables, understanding what they are likely used for, and inferring relationships between tables. It is quite good at writing JOINs between tables without explicit instruction.\n\nIt can write queries to group and summarize results.\n\nBy virtue of the Assistant API, it can also write multi-step queries and combine the results of queries together. It is also maintaining a history of the chat, so you can easily ask follow up questions like "now summarize by month" that refer back to prior queries.\n\nAt the moment the tool limits to only SELECT queries and does not allow mutations (but see the [SAFETY](#safety) section for details).\n\n## Chat examples\n\n```\n% gptsql\n\nWelcome to GPTSQL, the chat interface to your Postgres database.\n        \n> show me the available schemas\n⠋ thinking...  Running select query: SELECT schema_name FROM information_schema.schemata ORDER BY schema_name;\n[assistant] --> The available schemas in the database are:\n\n1. information_schema\n2. pg_catalog\n3. pg_toast\n4. public\n```\n\nShow tables matching a string:\n```\n> show all tables that have "sales" in the name\n⠴ thinking...  Running select query: SELECT table_name FROM information_schema.tables WHERE table_name LIKE \'%%sales%%\' ORDER BY table_name;\n[assistant] --> The tables with \'sales\' in the name are as follows:\n\n- salesorderdetail\n- salesorderheader\n- salesorderheadersalesreason\n- salesperson\n- salespersonquotahistory\n- salesreason\n- salestaxrate\n- salesterritory\n- salesterritoryhistory\n- vsalesperson\n- vsalespersonsalesbyfiscalyears\n- vsalespersonsalesbyfiscalyearsdata\n```\nList columns for a table:\n```\n> show the columns of the salesorderheader table\n⠹ thinking...  Running select query: SELECT column_name, data_type FROM information_schema.columns WHERE table_name = \'salesorderheader\' ORDER BY ordinal_position;\n[assistant] --> The columns of the `salesorderheader` table along with their data types are as follows:\n\n- `salesorderid` (integer)\n- `revisionnumber` (smallint)\n- `orderdate` (timestamp without time zone)\n- `duedate` (timestamp without time zone)\n- `shipdate` (timestamp without time zone)\n- `status` (smallint)\n- `subtotal` (numeric)\n- `taxamt` (numeric)\n- `freight` (numeric)\n- `totaldue` (numeric)\n- `comment` (character varying)\n- `rowguid` (uuid)\n- `modifieddate` (timestamp without time zone)\n```\nRun a calculation against a table:\n```\n> calculate the sum of the totaldue column by month\n⠇ thinking...  Running select query: SELECT DATE_TRUNC(\'month\', orderdate) AS month, SUM(totaldue) AS total_sales FROM sales.salesorderheader GROUP BY month ORDER BY month;\n[assistant] --> Here are the sums of the `totaldue` column by month:\n\nMonth               Total Sales\n-------------------------------------\n2011-05-01          567,020.95\n2011-06-01          507,096.47\n2011-07-01        2,292,182.88\n2011-08-01        2,800,576.17\n2011-09-01          554,791.61\n2014-02-01        1,478,213.29\n2014-03-01        8,097,036.31\n2014-04-01        1,985,886.15\n2014-05-01        6,006,183.21\n2014-06-01           54,151.48\nThese figures represent the total sales for each month.\n```\nQuery data from a table:\n```\n> show me the names of the stores\n⠹ thinking...  Running select query: SELECT name FROM sales.store;\n> print the results\n⠧ thinking...LAST QUERY RESULTS:\n⠇ thinking...                                    name\n0                   Next-Door Bike Store\n1         Professional Sales and Service\n2                         Riders Company\n3                     The Bike Mechanics\n4                      Nationwide Supply\n5                  Area Bike Accessories\n6           Bicycle Accessories and Kits\n7                  Clamps & Brackets Co.\n8             Valley Bicycle Specialists\n9                      New Bikes Company\n10   Vinyl and Plastic Goods Corporation\n11                 Top of the Line Bikes\n12                    Fun Toys and Bikes\n13                          Great Bikes \n14         Metropolitan Sales and Rental\n15                     Irregulars Outlet\n16                      Valley Toy Store\n17             Worthwhile Activity Store\n```\nExplain your data:\n```\n> what is the purpose of the salespersonquotahistory table?\n[assistant] --> The purpose of the `salespersonquotahistory` table is typically to maintain a record of the sales quota assigned to each salesperson over time. The table likely includes columns for:\n\n- Identifying the salesperson, which may reference a `businessentityid` or similar key that links to another table containing salesperson details.\n- The sales quota amount, which indicates the target sales figure the salesperson is expected to meet for a given period.\n- Date fields, which may specify the period or specific dates for the quota.\n- Any changes or historical adjustments made to the quota over time.\n\nIt serves as a historical log that can be used for performance tracking, compensation calculation, sales management, and analysis. The exact columns and usage may vary depending on the specific design of the database. If you have access to the table structure, you can review the columns to understand more about the data it holds. Would you like me to retrieve the columns of the `salespersonquotahistory` table to provide more detailed information?\n```\nEasily run custom Postgres SQL:\n```\n> show the top 10 tables by size on disk\n⠸ thinking...  Running select query: SELECT table_schema || \'.\' || table_name AS full_table_name,\npg_total_relation_size(\'"\' || table_schema || \'"."\' || table_name || \'"\') AS size_on_disk\nFROM information_schema.tables\nWHERE table_schema NOT IN (\'pg_catalog\', \'information_schema\')\nORDER BY size_on_disk DESC\nLIMIT 10;\n[assistant] --> The top 10 tables by size on disk are as follows:\n\n1. `sales.salesorderdetail`: 15,073,280 bytes\n2. `person.person`: 14,688,256 bytes\n3. `production.transactionhistory`: 11,304,960 bytes\n4. `production.workorderrouting`: 9,355,264 bytes\n5. `production.transactionhistoryarchive`: 8,896,512 bytes\n6. `production.workorder`: 7,823,360 bytes\n7. `sales.salesorderheader`: 6,676,480 bytes\n8. `production.productphoto`: 4,431,872 bytes\n9. `person.address`: 3,522,560 bytes\n10. `person.password`: 2,842,624 bytes\n\nThese sizes are in bytes, and the tables are listed in decreasing order of their size on disk.\n```\n\n### Configuration\n\nYou can configure the database connection either using `psql` style command line arguments\nor the env vars `DBHOST`, `DBNAME`, `DBUSER`, `DBPASSWORD`, `DBPORT`.\n\nElse when you first run the program it will prompt you for the connection credentials as\nwell as your OpenAI API key.\n\nAfter first setup all the configuration information is stored in `~/.gptsql`. Delete that\nfile if you want to start over.\n        \n## How it works\n\n`gptsql` uses the OpenAI [Assistants API](https://platform.openai.com/docs/assistants/overview) to create an intelligent assistant to work with your database.\nThe key to accessing the database is providing a function _tool_ to the assistant. Amazingly\nonly a single function is required:\n\n```json\n        {\n            "type": "function",\n            "function": {\n                "name": "run_sql_command",\n                "description": "Execute any SQL command against the Postgres datbase",\n            }\n        }\n```\n\nWhen requested the LLM automatically generates the right SQL and calls this function to execute it. The query results are then returned to the Assistant where it can decide to print or summarize the results. \n\nIf the LLM needs to know about your tables it will just execute SQL commands against the\n`information schema` to extract it. \n\nSince table reference is so common, we help the assistant by pre-emptively injecting the table\nlist into the LLM prompt.\n\nBecause of the LLM context limits, you won\'t always be able to see all the rows returned from a query.\nSo we provide a second function tool, `show_query_results` which can print up to 200 rows resulting from\nthe last query. Sometimes the assistant is smart enough to call this function by itself, but other times\nyou may have to request "print results" to see all the result rows.\n\n### Command Reference\n\nThere are a few system commands supported for meta operations: \n\n`help` - show system commands\n\n`connection` - show the current db connection details, and the active LLM model\n\n`history` - print the assistant\'s message history\n\n`new thread` - start a new thread (clearing out any existing conversation context)\n\n`exit` or ctrl-d to exit\n\nIf you want to change the LLM model you can edit the assistant via the OpenAI web portal.\n\n## SAFETY\n\n**Please do not run this against a production database!** And **make sure you have a backup** of your data. That said, the query function has a simple protector which will refuse to run any query that doesn\'t start with `SELECT`. Note that this is not foolproof. It is very likely that the LLM can construct a destructive query which will get around this simple check, if you ask it properly. So don\'t rely on this for perfect safety. I strongly recommend running with a `read-only` db connection just in case.\n\nAlso please note that this tool **sends your data to OpenAI**. Query results are sent back to Assistant API for processing. Please make your own decision on whether you are OK with this or not.\n\n## Limitations\n\nThe biggest limitation is that the LLM is **slooooowwww**. In my testing it can easily take 20-30 seconds of "thinking" before the LLM responds. One reason is that the LLM runs once when you issue your question, and then it runs again to process any results returned from the functions. \n\nIt is also the case the the Assistants API will run multiple "steps" to process your question, even though we don\'t get a lot of feedback when this is happening.\n\nOne other thing: the tool is **expensive** :). I ran up a bill of about $100 just doing development. It is recommnded to stick with the **GPT3** model if you want to keep your costs down.\n',
    'author': 'Scott Persinger',
    'author_email': 'scottpersinger@gmail.com',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'entry_points': entry_points,
    'python_requires': '>=3.9,<4.0',
}


setup(**setup_kwargs)
