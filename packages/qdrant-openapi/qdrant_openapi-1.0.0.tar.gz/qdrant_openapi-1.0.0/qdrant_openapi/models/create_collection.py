# coding: utf-8

"""
    Qdrant API

    API description for Qdrant vector search engine.  This document describes CRUD and search operations on collections of points (vectors with payload).  Qdrant supports any combinations of `should`, `must` and `must_not` conditions, which makes it possible to use in applications when object could not be described solely by vector. It could be location features, availability flags, and other custom properties businesses should take into account. ## Examples This examples cover the most basic use-cases - collection creation and basic vector search. ### Create collection First - let's create a collection with dot-production metric. ``` curl -X PUT 'http://localhost:6333/collections/test_collection' \\   -H 'Content-Type: application/json' \\   --data-raw '{     \"vectors\": {       \"size\": 4,       \"distance\": \"Dot\"     }   }'  ``` Expected response: ``` {     \"result\": true,     \"status\": \"ok\",     \"time\": 0.031095451 } ``` We can ensure that collection was created: ``` curl 'http://localhost:6333/collections/test_collection' ``` Expected response: ``` {   \"result\": {     \"status\": \"green\",     \"vectors_count\": 0,     \"segments_count\": 5,     \"disk_data_size\": 0,     \"ram_data_size\": 0,     \"config\": {       \"params\": {         \"vectors\": {           \"size\": 4,           \"distance\": \"Dot\"         }       },       \"hnsw_config\": {         \"m\": 16,         \"ef_construct\": 100,         \"full_scan_threshold\": 10000       },       \"optimizer_config\": {         \"deleted_threshold\": 0.2,         \"vacuum_min_vector_number\": 1000,         \"max_segment_number\": 5,         \"memmap_threshold\": 50000,         \"indexing_threshold\": 20000,         \"flush_interval_sec\": 1       },       \"wal_config\": {         \"wal_capacity_mb\": 32,         \"wal_segments_ahead\": 0       }     }   },   \"status\": \"ok\",   \"time\": 2.1199e-05 } ```  ### Add points Let's now add vectors with some payload: ``` curl -L -X PUT 'http://localhost:6333/collections/test_collection/points?wait=true' \\ -H 'Content-Type: application/json' \\ --data-raw '{   \"points\": [     {\"id\": 1, \"vector\": [0.05, 0.61, 0.76, 0.74], \"payload\": {\"city\": \"Berlin\"}},     {\"id\": 2, \"vector\": [0.19, 0.81, 0.75, 0.11], \"payload\": {\"city\": [\"Berlin\", \"London\"] }},     {\"id\": 3, \"vector\": [0.36, 0.55, 0.47, 0.94], \"payload\": {\"city\": [\"Berlin\", \"Moscow\"] }},     {\"id\": 4, \"vector\": [0.18, 0.01, 0.85, 0.80], \"payload\": {\"city\": [\"London\", \"Moscow\"] }},     {\"id\": 5, \"vector\": [0.24, 0.18, 0.22, 0.44], \"payload\": {\"count\": [0]}},     {\"id\": 6, \"vector\": [0.35, 0.08, 0.11, 0.44]}   ] }' ``` Expected response: ``` {     \"result\": {         \"operation_id\": 0,         \"status\": \"completed\"     },     \"status\": \"ok\",     \"time\": 0.000206061 } ``` ### Search with filtering Let's start with a basic request: ``` curl -L -X POST 'http://localhost:6333/collections/test_collection/points/search' \\ -H 'Content-Type: application/json' \\ --data-raw '{     \"vector\": [0.2,0.1,0.9,0.7],     \"top\": 3 }' ``` Expected response: ``` {     \"result\": [         { \"id\": 4, \"score\": 1.362, \"payload\": null, \"version\": 0 },         { \"id\": 1, \"score\": 1.273, \"payload\": null, \"version\": 0 },         { \"id\": 3, \"score\": 1.208, \"payload\": null, \"version\": 0 }     ],     \"status\": \"ok\",     \"time\": 0.000055785 } ``` But result is different if we add a filter: ``` curl -L -X POST 'http://localhost:6333/collections/test_collection/points/search' \\ -H 'Content-Type: application/json' \\ --data-raw '{     \"filter\": {         \"should\": [             {                 \"key\": \"city\",                 \"match\": {                     \"value\": \"London\"                 }             }         ]     },     \"vector\": [0.2, 0.1, 0.9, 0.7],     \"top\": 3 }' ``` Expected response: ``` {     \"result\": [         { \"id\": 4, \"score\": 1.362, \"payload\": null, \"version\": 0 },         { \"id\": 2, \"score\": 0.871, \"payload\": null, \"version\": 0 }     ],     \"status\": \"ok\",     \"time\": 0.000093972 } ``` 

    The version of the OpenAPI document: v1.7.x
    Contact: andrey@vasnetsov.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json


from typing import Dict, Optional
from pydantic import BaseModel, Field, StrictBool, conint
from qdrant_openapi.models.hnsw_config_diff import HnswConfigDiff
from qdrant_openapi.models.init_from import InitFrom
from qdrant_openapi.models.optimizers_config_diff import OptimizersConfigDiff
from qdrant_openapi.models.quantization_config import QuantizationConfig
from qdrant_openapi.models.sharding_method import ShardingMethod
from qdrant_openapi.models.sparse_vector_params import SparseVectorParams
from qdrant_openapi.models.vectors_config import VectorsConfig
from qdrant_openapi.models.wal_config_diff import WalConfigDiff

class CreateCollection(BaseModel):
    """
    Operation for creating new collection and (optionally) specify index params  # noqa: E501
    """
    vectors: Optional[VectorsConfig] = None
    shard_number: Optional[conint(strict=True, ge=1)] = Field(None, description="For auto sharding: Number of shards in collection. - Default is 1 for standalone, otherwise equal to the number of nodes - Minimum is 1 For custom sharding: Number of shards in collection per shard group. - Default is 1, meaning that each shard key will be mapped to a single shard - Minimum is 1")
    sharding_method: Optional[ShardingMethod] = None
    replication_factor: Optional[conint(strict=True, ge=1)] = Field(None, description="Number of shards replicas. Default is 1 Minimum is 1")
    write_consistency_factor: Optional[conint(strict=True, ge=1)] = Field(None, description="Defines how many replicas should apply the operation for us to consider it successful. Increasing this number will make the collection more resilient to inconsistencies, but will also make it fail if not enough replicas are available. Does not have any performance impact.")
    on_disk_payload: Optional[StrictBool] = Field(None, description="If true - point's payload will not be stored in memory. It will be read from the disk every time it is requested. This setting saves RAM by (slightly) increasing the response time. Note: those payload values that are involved in filtering and are indexed - remain in RAM.")
    hnsw_config: Optional[HnswConfigDiff] = None
    wal_config: Optional[WalConfigDiff] = None
    optimizers_config: Optional[OptimizersConfigDiff] = None
    init_from: Optional[InitFrom] = None
    quantization_config: Optional[QuantizationConfig] = None
    sparse_vectors: Optional[Dict[str, SparseVectorParams]] = Field(None, description="Sparse vector data config.")
    __properties = ["vectors", "shard_number", "sharding_method", "replication_factor", "write_consistency_factor", "on_disk_payload", "hnsw_config", "wal_config", "optimizers_config", "init_from", "quantization_config", "sparse_vectors"]

    class Config:
        """Pydantic configuration"""
        allow_population_by_field_name = True
        validate_assignment = True

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.dict(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> CreateCollection:
        """Create an instance of CreateCollection from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self):
        """Returns the dictionary representation of the model using alias"""
        _dict = self.dict(by_alias=True,
                          exclude={
                          },
                          exclude_none=True)
        # override the default output from pydantic by calling `to_dict()` of vectors
        if self.vectors:
            _dict['vectors'] = self.vectors.to_dict()
        # override the default output from pydantic by calling `to_dict()` of hnsw_config
        if self.hnsw_config:
            _dict['hnsw_config'] = self.hnsw_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of wal_config
        if self.wal_config:
            _dict['wal_config'] = self.wal_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of optimizers_config
        if self.optimizers_config:
            _dict['optimizers_config'] = self.optimizers_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of init_from
        if self.init_from:
            _dict['init_from'] = self.init_from.to_dict()
        # override the default output from pydantic by calling `to_dict()` of quantization_config
        if self.quantization_config:
            _dict['quantization_config'] = self.quantization_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each value in sparse_vectors (dict)
        _field_dict = {}
        if self.sparse_vectors:
            for _key in self.sparse_vectors:
                if self.sparse_vectors[_key]:
                    _field_dict[_key] = self.sparse_vectors[_key].to_dict()
            _dict['sparse_vectors'] = _field_dict
        # set to None if shard_number (nullable) is None
        # and __fields_set__ contains the field
        if self.shard_number is None and "shard_number" in self.__fields_set__:
            _dict['shard_number'] = None

        # set to None if sharding_method (nullable) is None
        # and __fields_set__ contains the field
        if self.sharding_method is None and "sharding_method" in self.__fields_set__:
            _dict['sharding_method'] = None

        # set to None if replication_factor (nullable) is None
        # and __fields_set__ contains the field
        if self.replication_factor is None and "replication_factor" in self.__fields_set__:
            _dict['replication_factor'] = None

        # set to None if write_consistency_factor (nullable) is None
        # and __fields_set__ contains the field
        if self.write_consistency_factor is None and "write_consistency_factor" in self.__fields_set__:
            _dict['write_consistency_factor'] = None

        # set to None if on_disk_payload (nullable) is None
        # and __fields_set__ contains the field
        if self.on_disk_payload is None and "on_disk_payload" in self.__fields_set__:
            _dict['on_disk_payload'] = None

        # set to None if hnsw_config (nullable) is None
        # and __fields_set__ contains the field
        if self.hnsw_config is None and "hnsw_config" in self.__fields_set__:
            _dict['hnsw_config'] = None

        # set to None if wal_config (nullable) is None
        # and __fields_set__ contains the field
        if self.wal_config is None and "wal_config" in self.__fields_set__:
            _dict['wal_config'] = None

        # set to None if optimizers_config (nullable) is None
        # and __fields_set__ contains the field
        if self.optimizers_config is None and "optimizers_config" in self.__fields_set__:
            _dict['optimizers_config'] = None

        # set to None if init_from (nullable) is None
        # and __fields_set__ contains the field
        if self.init_from is None and "init_from" in self.__fields_set__:
            _dict['init_from'] = None

        # set to None if quantization_config (nullable) is None
        # and __fields_set__ contains the field
        if self.quantization_config is None and "quantization_config" in self.__fields_set__:
            _dict['quantization_config'] = None

        # set to None if sparse_vectors (nullable) is None
        # and __fields_set__ contains the field
        if self.sparse_vectors is None and "sparse_vectors" in self.__fields_set__:
            _dict['sparse_vectors'] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: dict) -> CreateCollection:
        """Create an instance of CreateCollection from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return CreateCollection.parse_obj(obj)

        _obj = CreateCollection.parse_obj({
            "vectors": VectorsConfig.from_dict(obj.get("vectors")) if obj.get("vectors") is not None else None,
            "shard_number": obj.get("shard_number"),
            "sharding_method": obj.get("sharding_method"),
            "replication_factor": obj.get("replication_factor"),
            "write_consistency_factor": obj.get("write_consistency_factor"),
            "on_disk_payload": obj.get("on_disk_payload"),
            "hnsw_config": HnswConfigDiff.from_dict(obj.get("hnsw_config")) if obj.get("hnsw_config") is not None else None,
            "wal_config": WalConfigDiff.from_dict(obj.get("wal_config")) if obj.get("wal_config") is not None else None,
            "optimizers_config": OptimizersConfigDiff.from_dict(obj.get("optimizers_config")) if obj.get("optimizers_config") is not None else None,
            "init_from": InitFrom.from_dict(obj.get("init_from")) if obj.get("init_from") is not None else None,
            "quantization_config": QuantizationConfig.from_dict(obj.get("quantization_config")) if obj.get("quantization_config") is not None else None,
            "sparse_vectors": dict(
                (_k, SparseVectorParams.from_dict(_v))
                for _k, _v in obj.get("sparse_vectors").items()
            )
            if obj.get("sparse_vectors") is not None
            else None
        })
        return _obj


